{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d9a4793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Found 0 AF files\n",
      "Found 0 non-AF files\n",
      "Extracting features from 0 AF files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from 0 non-AF files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction completed!\n",
      "Total samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 125\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFeature extraction completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    124\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAF samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_features[\u001b[43mall_features\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m==\u001b[32m1\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNon-AF samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_features[all_features[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]==\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# 3. Chuẩn bị dữ liệu cho ML\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'label'"
     ]
    }
   ],
   "source": [
    "# Cell hoàn chỉnh: Load data, extract features và test Random Forest\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from scipy.stats import skew, kurtosis\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load dữ liệu\n",
    "print(\"Loading data...\")\n",
    "def load_data(af_folder, non_af_folder):\n",
    "    af_files = glob.glob(f\"{af_folder}/*.csv\")\n",
    "    non_af_files = glob.glob(f\"{non_af_folder}/*.csv\")\n",
    "    \n",
    "    print(f\"Found {len(af_files)} AF files\")\n",
    "    print(f\"Found {len(non_af_files)} non-AF files\")\n",
    "    \n",
    "    # Load AF data\n",
    "    af_data = []\n",
    "    for file in af_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df['label'] = 1  # AF = 1\n",
    "        df['file_id'] = Path(file).stem\n",
    "        af_data.append(df)\n",
    "    \n",
    "    # Load non-AF data\n",
    "    non_af_data = []\n",
    "    for file in non_af_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df['label'] = 0  # non-AF = 0\n",
    "        df['file_id'] = Path(file).stem\n",
    "        non_af_data.append(df)\n",
    "    \n",
    "    return af_data, non_af_data\n",
    "\n",
    "# Load data\n",
    "af_data, non_af_data = load_data(\n",
    "    \"DATA AF CSV/mimic_perform_af_csv\", \n",
    "    \"DATA AF CSV/mimic_perform_non_af_csv\"\n",
    ")\n",
    "\n",
    "# 2. Trích xuất đặc trưng\n",
    "def extract_features(ppg_signal, fs=125):\n",
    "    \"\"\"Simple and fast feature extraction from PPG signal\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Basic statistical features\n",
    "    features['mean'] = np.mean(ppg_signal)\n",
    "    features['std'] = np.std(ppg_signal)\n",
    "    features['min'] = np.min(ppg_signal)\n",
    "    features['max'] = np.max(ppg_signal)\n",
    "    features['range'] = features['max'] - features['min']\n",
    "    features['skewness'] = skew(ppg_signal)\n",
    "    features['kurtosis'] = kurtosis(ppg_signal)\n",
    "    features['rms'] = np.sqrt(np.mean(ppg_signal**2))\n",
    "    \n",
    "    # Peak-based features\n",
    "    try:\n",
    "        peaks, _ = signal.find_peaks(ppg_signal, height=features['mean'], distance=fs//3)\n",
    "        features['num_peaks'] = len(peaks)\n",
    "        \n",
    "        if len(peaks) > 1:\n",
    "            rr_intervals = np.diff(peaks) / fs\n",
    "            features['mean_hr'] = 60 / np.mean(rr_intervals)\n",
    "            features['std_hr'] = np.std(60 / rr_intervals)\n",
    "            features['rmssd'] = np.sqrt(np.mean(np.diff(rr_intervals)**2))\n",
    "        else:\n",
    "            features.update({'mean_hr': 0, 'std_hr': 0, 'rmssd': 0})\n",
    "    except:\n",
    "        features.update({'num_peaks': 0, 'mean_hr': 0, 'std_hr': 0, 'rmssd': 0})\n",
    "    \n",
    "    # Frequency domain features\n",
    "    try:\n",
    "        f, psd = signal.welch(ppg_signal, fs=fs, nperseg=min(256, len(ppg_signal)//4))\n",
    "        \n",
    "        lf_mask = (f >= 0.04) & (f < 0.15)\n",
    "        hf_mask = (f >= 0.15) & (f < 0.4)\n",
    "        \n",
    "        features['lf_power'] = np.trapz(psd[lf_mask], f[lf_mask]) if np.any(lf_mask) else 0\n",
    "        features['hf_power'] = np.trapz(psd[hf_mask], f[hf_mask]) if np.any(hf_mask) else 0\n",
    "        features['lf_hf_ratio'] = features['lf_power'] / (features['hf_power'] + 1e-6)\n",
    "        features['dominant_freq'] = f[np.argmax(psd)]\n",
    "    except:\n",
    "        features.update({'lf_power': 0, 'hf_power': 0, 'lf_hf_ratio': 0, 'dominant_freq': 0})\n",
    "    \n",
    "    # Other simple features\n",
    "    features['zero_crossings'] = len(np.where(np.diff(np.sign(ppg_signal - features['mean'])))[0])\n",
    "    features['energy'] = np.sum(ppg_signal**2)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_all_features(data_list, label_name=\"\"):\n",
    "    feature_list = []\n",
    "    print(f\"Extracting features from {len(data_list)} {label_name} files...\")\n",
    "    \n",
    "    for df in tqdm(data_list):\n",
    "        try:\n",
    "            features = extract_features(df['PPG'].values)\n",
    "            features['label'] = df['label'].iloc[0]\n",
    "            features['file_id'] = df['file_id'].iloc[0]\n",
    "            feature_list.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(feature_list)\n",
    "\n",
    "# Extract features\n",
    "af_features = extract_all_features(af_data, \"AF\")\n",
    "non_af_features = extract_all_features(non_af_data, \"non-AF\")\n",
    "all_features = pd.concat([af_features, non_af_features], ignore_index=True)\n",
    "\n",
    "print(f\"\\nFeature extraction completed!\")\n",
    "print(f\"Total samples: {len(all_features)}\")\n",
    "print(f\"AF samples: {len(all_features[all_features['label']==1])}\")\n",
    "print(f\"Non-AF samples: {len(all_features[all_features['label']==0])}\")\n",
    "\n",
    "# 3. Chuẩn bị dữ liệu cho ML\n",
    "feature_columns = [col for col in all_features.columns if col not in ['label', 'file_id']]\n",
    "X = all_features[feature_columns]\n",
    "y = all_features['label']\n",
    "\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(f\"Features: {feature_columns}\")\n",
    "\n",
    "# Xử lý missing values và chuẩn hóa\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Chia dữ liệu train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# 4. Huấn luyện Random Forest\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Dự đoán và đánh giá\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RANDOM FOREST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Non-AF', 'AF']))\n",
    "\n",
    "# 6. Cross-validation\n",
    "cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_auc_scores = cross_val_score(rf_model, X_scaled, y, cv=cv_folds, scoring='roc_auc')\n",
    "cv_acc_scores = cross_val_score(rf_model, X_scaled, y, cv=cv_folds, scoring='accuracy')\n",
    "\n",
    "print(f\"\\nCross-validation AUC: {cv_auc_scores.mean():.4f} (+/- {cv_auc_scores.std() * 2:.4f})\")\n",
    "print(f\"Cross-validation Accuracy: {cv_acc_scores.mean():.4f} (+/- {cv_acc_scores.std() * 2:.4f})\")\n",
    "\n",
    "# 7. Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 most important features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# 8. Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
    "axes[0,0].set_title('Confusion Matrix')\n",
    "axes[0,0].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "axes[0,1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {test_auc:.3f})')\n",
    "axes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[0,1].set_xlim([0.0, 1.0])\n",
    "axes[0,1].set_ylim([0.0, 1.05])\n",
    "axes[0,1].set_xlabel('False Positive Rate')\n",
    "axes[0,1].set_ylabel('True Positive Rate')\n",
    "axes[0,1].set_title('ROC Curve')\n",
    "axes[0,1].legend(loc=\"lower right\")\n",
    "\n",
    "# Feature Importance\n",
    "top_features = feature_importance.head(10)\n",
    "axes[1,0].barh(range(len(top_features)), top_features['importance'])\n",
    "axes[1,0].set_yticks(range(len(top_features)))\n",
    "axes[1,0].set_yticklabels(top_features['feature'])\n",
    "axes[1,0].set_xlabel('Importance')\n",
    "axes[1,0].set_title('Top 10 Feature Importance')\n",
    "axes[1,0].invert_yaxis()\n",
    "\n",
    "# CV scores\n",
    "axes[1,1].boxplot([cv_auc_scores, cv_acc_scores], labels=['AUC', 'Accuracy'])\n",
    "axes[1,1].set_title('Cross-validation Scores')\n",
    "axes[1,1].set_ylabel('Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model: Random Forest\")\n",
    "print(f\"Features: {len(feature_columns)}\")\n",
    "print(f\"Total samples: {len(all_features)}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "print(f\"CV AUC: {cv_auc_scores.mean():.4f} ± {cv_auc_scores.std():.4f}\")\n",
    "\n",
    "if cv_auc_scores.mean() > 0.8:\n",
    "    print(\"✅ Model performance: EXCELLENT\")\n",
    "elif cv_auc_scores.mean() > 0.7:\n",
    "    print(\"⚠️ Model performance: GOOD\")\n",
    "else:\n",
    "    print(\"❌ Model performance: NEEDS IMPROVEMENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0bc9e7",
   "metadata": {},
   "source": [
    "EXPORT DATA TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1535ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Chuyển X_train thành DataFrame với tên cột = feature_columns\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_train_df = pd.DataFrame(\u001b[43mX_train\u001b[49m, columns=feature_columns)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Thêm cột label từ y_train\u001b[39;00m\n\u001b[32m      5\u001b[39m X_train_df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] = y_train.values \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y_train, \u001b[33m'\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m y_train\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Chuyển X_train thành DataFrame với tên cột = feature_columns\n",
    "X_train_df = pd.DataFrame(X_train, columns=feature_columns)\n",
    "\n",
    "# Thêm cột label từ y_train\n",
    "X_train_df['label'] = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "\n",
    "# Lưu X_train + y_train thành file CSV\n",
    "X_train_df.to_csv(\"X_train_y_train.csv\", index=False)\n",
    "\n",
    "# Lưu feature_columns thành file CSV riêng\n",
    "pd.DataFrame({'feature': feature_columns}).to_csv(\"feature_columns.csv\", index=False)\n",
    "\n",
    "print(\"✅ Đã lưu X_train_y_train.csv và feature_columns.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
